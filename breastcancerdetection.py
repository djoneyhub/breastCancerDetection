# -*- coding: utf-8 -*-
"""BreastCancerDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vNgQU67JjDxVQXbXx2UNc0jf8pC51wKG
"""

# handle imports

import seaborn as sns # plotting
import tensorflow as tf # Machine Learning Library
import numpy as np # Mathematical Computation
import pathlib # for representing the imagepaths
import os # system

import pandas as pd
import glob
import re

# define parameters
BATCH_SIZE = 32
EPOCHS = 50
LR = 0.01
IMAGE_SIZE = (256, 256)

sns.set()

print(tf.__version__)

"""#Image Upload"""

import zipfile
import pathlib

from google.colab import drive

drive.mount('/content/drive')

from pathlib import Path

data_dir = Path('/content/drive/MyDrive/ultrasound_dataset')  # Replace with the path to your desired directory

classes = [item.name for item in data_dir.iterdir() if item.is_dir()]
print(classes)

import os

"""#preparation

##benign dataframe
"""

import re
from PIL import Image

benign_path = os.path.join(data_dir,"benign")

ben_label = []
benign_images = sorted(glob.glob(benign_path +"/*).png"))
benign_masks = sorted(glob.glob(benign_path +"/*mask.png"))
key = [int(re.findall(r'[0-9]+',image_name)[0]) for image_name in benign_images]
for lab in benign_images:
  ben_label.append(1)

image_sizes = []
for image_path in benign_images:
    with Image.open(image_path) as img:
        width, height = img.size
        image_sizes.append((width, height))

benign_df = pd.DataFrame({'key':key,'label':ben_label,'images':benign_images,'masks':benign_masks,'sizes': image_sizes})
benign_df

"""##malignant dataframe"""

mal_path = os.path.join(data_dir,"malignant")

mal_label = []
mal_images = sorted(glob.glob(mal_path +"/*).png"))
mal_masks = sorted(glob.glob(mal_path +"/*mask.png"))
mal_key = [int(re.findall(r'[0-9]+',image_name)[0]) for image_name in mal_images]
for lab in mal_images:
  mal_label.append(2)

mal_sizes = []
for image_path in mal_images:
    with Image.open(image_path) as img:
        width, height = img.size
        mal_sizes.append((width, height))

mal_df = pd.DataFrame({'key':mal_key,'label':mal_label,'images':mal_images,'masks':mal_masks,'sizes': mal_sizes})
mal_df

"""##normal dataframe"""

norm_path = os.path.join(data_dir,"normal")

norm_label = []
norm_images = sorted(glob.glob(norm_path +"/*).png"))
norm_masks = sorted(glob.glob(norm_path +"/*mask.png"))
norm_key = [int(re.findall(r'[0-9]+',image_name)[0]) for image_name in norm_images]
for lab in norm_images:
  norm_label.append(0)

norm_sizes = []
for image_path in norm_images:
    with Image.open(image_path) as img:
        width, height = img.size
        norm_sizes.append((width, height))

norm_df = pd.DataFrame({'key':norm_key,'label':norm_label,'images':norm_images,'masks':norm_masks,'sizes': norm_sizes})
norm_df

"""##master dataframe"""

cancer_df = pd.concat([benign_df,mal_df,norm_df])
cancer_df

"""#DATA VISUALISATION"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")

label_bar = sns.countplot(x = cancer_df["label"])
for label in label_bar.containers:
    label_bar.bar_label(label)
plt.title('Data Distribution')
plt.show(label_bar)

"""#Pre-Processing

##labeling images
"""

def get_image_lists(cancer_df, image_size):

    image_lists = cancer_df['images'].values.tolist()
    image_label = cancer_df['label'].values.tolist()

    # Create a TensorFlow dataset from the list
    image_lists = tf.data.Dataset.from_tensor_slices(image_lists)
    image_label = tf.data.Dataset.from_tensor_slices(image_label)

    # Define the preprocessing function inside the map
    def preprocess_and_extract_label(image_path):
        # Read the image from the given path
        img = tf.io.read_file(image_path)
        # Decode the image and change to RGB
        img = tf.image.decode_png(img, channels=3)

        # Resize the image
        img = tf.image.resize(img, image_size)

        # Normalize the image pixel values to the range [0, 1]
        img = tf.cast(img, tf.float32) / 255.0

        return img

    # Apply preprocessing and label extraction using the lambda function
    image_lists = image_lists = image_lists.map(lambda image_path: preprocess_and_extract_label(image_path))
    image_lists = tf.data.Dataset.zip((image_lists, image_label))


    return image_lists

image_lists = get_image_lists(cancer_df, IMAGE_SIZE)

img_count = len(image_lists)
print("the dataset contains:", img_count)
# Take the first two elements from the image_lists dataset
two_images = image_lists.take(2)
# Iterate through the image_lists dataset and print the image data and image path
for image, label in two_images:
    print("Image Data:")
    print(image)

    print("Image Label:")
    print(label)


    import matplotlib.pyplot as plt
    plt.imshow(image.numpy())
    plt.show()

"""##augmentation"""



from collections import Counter

def data_augmentation(image, label):

    # Randomly apply horizontal flip with a probability of 0.5
    image = tf.image.random_flip_left_right(image)
    # Randomly apply rotation with a maximum angle of 45 degrees
    image = np.rot90(image, 45)


    return image, label

# Convert the TensorFlow dataset to a pandas DataFrame
image_data = []
image_labels = []
for image, label in image_lists:
    image_data.append(image.numpy())
    image_labels.append(label.numpy())

data = {'images': image_data, 'label': image_labels}
df = pd.DataFrame(data)

# Perform data augmentation for classes 0 and 2
augmented_data = []
for index, row in df.iterrows():
    if row['label'] in [0, 2]:
        augmented_image, label = data_augmentation(row['images'], row['label'])
        augmented_data.append({'images': augmented_image, 'label': label})
augmented_df = df.append(augmented_data, ignore_index=True)

# Balance the dataset to have 500 samples per class
balanced_df = pd.DataFrame()
for label in [0, 1, 2]:
    class_data = augmented_df[augmented_df['label'] == label]
    if len(class_data) < 500:
        class_data = class_data.sample(n=500, replace=True)
    balanced_df = pd.concat([balanced_df, class_data], ignore_index=True)

# Convert the balanced DataFrame back to a TensorFlow dataset
balanced_image_data = np.array(balanced_df['images'].tolist())
balanced_image_labels = np.array(balanced_df['label'].tolist(), dtype=np.int32)
balanced_dataset = tf.data.Dataset.from_tensor_slices((balanced_image_data, balanced_image_labels))

# Display some examples of augmented and balanced images
num_examples_to_display = 4
for image, label in balanced_dataset.take(num_examples_to_display):
    print(image.shape)
    print(label.shape)
    plt.imshow(image.numpy())
    plt.title(f"Label: {label.numpy()}")
    plt.show()

num_images_to_plot = 15

# Shuffle the dataset to randomize the order
shuffled_dataset = balanced_dataset.shuffle(buffer_size=len(balanced_dataset))

# Create a figure to display the images
plt.figure(figsize=(12, 8))

# Counter to keep track of the plotted images
plot_counter = 0

# Iterate through the shuffled dataset and plot images with their corresponding labels
for image, label in shuffled_dataset:
    if plot_counter >= num_images_to_plot:
        break

    # Increment the plot counter
    plot_counter += 1

    # Create a new subplot
    plt.subplot(3, 5, plot_counter)

    # Show the image
    plt.imshow(image.numpy())

    # Set the title to the corresponding label
    plt.title(f"Label: {label.numpy()}")
    plt.axis("off")

plt.tight_layout()
plt.show()

beningn_cnt= 0
normal_cnt= 0
mal_cnt = 0
for image, label in balanced_dataset:
    if label.numpy() == 0 :
      normal_cnt+=1
    elif label.numpy() == 1:
      beningn_cnt+=1
    elif label.numpy() == 2:
      mal_cnt+=1
    else:
      print("wrong label")

print("count of normal data:")
print(normal_cnt)
print("--------------------")
print("count of benign data:")
print(beningn_cnt)
print("--------------------")
print("count of malignant data:")
print(mal_cnt)
print("--------------------")

"""##batching"""

BATCH_SIZE_TRAIN = 32
BATCH_SIZE_TEST = 32
BATCH_SIZE_VAL= 32
num_classes = 3

from sklearn.model_selection import train_test_split

num_classes = 3
img_count = len(balanced_dataset)
test_size = int(0.2 * img_count)  # 20% of the data for testing
val_size = int(0.1 * img_count)  # 10% of the data for validation

# Shuffle the entire dataset
balanced_dataset = balanced_dataset.shuffle(buffer_size=img_count, seed=42)

# Split the dataset into train, val, and test sets
train_data = balanced_dataset.take(img_count - test_size - val_size)
val_data = balanced_dataset.skip(img_count - test_size).take(val_size)
test_data = balanced_dataset.skip(img_count - test_size)

# Batching function
BATCH_SIZE_TRAIN = 32
BATCH_SIZE_TEST = 32
BATCH_SIZE_VAL = 32

def configure_for_training(data, is_train=False, batch_size=32):
    data = data.cache()
    if is_train:
        data = data.shuffle(buffer_size=300)
    data = data.batch(batch_size, drop_remainder=True)
    data = data.prefetch(buffer_size=tf.data.AUTOTUNE)
    return data

train_data = configure_for_training(train_data, is_train=True, batch_size=BATCH_SIZE_TRAIN)
val_data = configure_for_training(val_data, is_train=False, batch_size=BATCH_SIZE_VAL)
test_data = configure_for_training(test_data, is_train=False, batch_size=BATCH_SIZE_TEST)

for image_batch, label_batch in train_data.take(1):
    print("Training Batch:")
    print(image_batch.shape)
    print(label_batch.shape)

# Perform the counting on train_data
normal_cnt = 0
benign_cnt = 0
malignant_cnt = 0

for _, label in train_data:
  for label in label_batch.numpy():
    if label == 0:
        normal_cnt += 1
    elif label == 1:
        benign_cnt += 1
    elif label == 2:
        malignant_cnt += 1
    else:
        print("Wrong label")

print("Count of normal data in train_data:", normal_cnt)
print("Count of benign data in train_data:", benign_cnt)
print("Count of malignant data in train_data:", malignant_cnt)
print("--------------------")

for image_batch, label_batch in val_data.take(1):
    print("Validation Batch:")
    print(image_batch.shape)
    print(label_batch.shape)

normal_cnt = 0
benign_cnt = 0
malignant_cnt = 0

for _, label in val_data:
  for label in label_batch.numpy():
    if label == 0:
        normal_cnt += 1
    elif label == 1:
        benign_cnt += 1
    elif label == 2:
        malignant_cnt += 1
    else:
        print("Wrong label")

print("Count of normal data in val_data:", normal_cnt)
print("Count of benign data in val_data:", benign_cnt)
print("Count of malignant data in val_data:", malignant_cnt)
print("--------------------")

for image_batch, label_batch in test_data.take(1):
    print("Testing Batch:")
    print(image_batch.shape)
    print(label_batch.shape)

normal_cnt = 0
benign_cnt = 0
malignant_cnt = 0

for _, label in test_data:
  for label in label_batch.numpy():
    if label == 0:
        normal_cnt += 1
    elif label == 1:
        benign_cnt += 1
    elif label == 2:
        malignant_cnt += 1
    else:
        print("Wrong label")

print("Count of normal data in train_data:", normal_cnt)
print("Count of benign data in train_data:", benign_cnt)
print("Count of malignant data in train_data:", malignant_cnt)
print("--------------------")

"""#TRAINING AND TESTING

##TRAINING
"""

def get_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(256, 256, 3), activation="relu"),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.BatchNormalization(),  # Add Batch Normalization after the first Conv2D
        tf.keras.layers.Conv2D(64, (3, 3), activation="relu"),  # Add another Conv2D layer
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.BatchNormalization(),  # Add Batch Normalization after the second Conv2D
        tf.keras.layers.Conv2D(128, (3, 3), activation="relu"),  # Add a third Conv2D layer
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.BatchNormalization(),  # Add Batch Normalization after the third Conv2D
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),  # Add Dropout regularization to reduce overfitting
        tf.keras.layers.Dense(256, activation="relu"),  # Add a dense layer with more units
        tf.keras.layers.Dense(3, activation="softmax")
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Reduce the learning rate
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy])

    return model


model = get_model()
model.summary()

"""##testing"""

# fit the model
history = model.fit(
    train_data,
    epochs=50,
    validation_data=val_data,
)

"""#Validation"""

# visualize the training and validation accuracies and losses
def visualize_loss(history):
    train_loss = history["loss"]
    val_loss = history["val_loss"]

    iterations = [i for i in range(EPOCHS)]

    plt.plot(iterations, train_loss, label="Training Loss", color="b")
    plt.plot(iterations, val_loss, label="Validation Loss", color="r")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.show()

def visualize_acc(history):
    train_loss = history["sparse_categorical_accuracy"]
    val_loss = history["val_sparse_categorical_accuracy"]

    iterations = [i for i in range(EPOCHS)]


    plt.plot(iterations, train_loss, label="Training Accuracy", color="b")
    plt.plot(iterations, val_loss, label="Validation Accuracy", color="r")
    plt.title("Training and Validation accuracy")
    plt.legend()
    plt.show()

visualize_loss(history.history)
visualize_acc(history.history)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Once training is done, compute the evaluation metrics on the test data
y_pred = model.predict(test_data)
predicted_labels = np.argmax(y_pred, axis=1)

true_labels = np.concatenate([y for x, y in test_data], axis=0)

accuracy = model.evaluate(test_data, verbose=0)[1]
precision = precision_score(true_labels, predicted_labels, average='macro')
recall = recall_score(true_labels, predicted_labels, average='macro')
f1 = f1_score(true_labels, predicted_labels, average='macro')
sensitivity = recall  # Same as recall in binary classification

print(f"Test Accuracy: {accuracy:.4f}\n")
print(f"Precision: {precision:.4f}\n Recall: {recall:.4f}\n F1 Score: {f1:.4f}")

from sklearn.metrics import confusion_matrix

# Assuming you have already trained the model and have test_data
# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_data)

# Get the predictions from the model for the test data
test_predictions = model.predict(test_data)

# Get the true labels for the test data
test_true_labels = []
for _, label_batch in test_data:
    test_true_labels.extend(label_batch.numpy())

# Calculate the confusion matrix
conf_matrix = confusion_matrix(test_true_labels, test_predictions.argmax(axis=1))

print("Confusion Matrix:")
print(conf_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Class 0", "Class 1", "Class 2"], yticklabels=["Class 0", "Class 1", "Class 2"])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

"""#download mode

"""

# Save the model for inference
model.save("breast_cancer_classification_model.h5")

from google.colab import files
files.download("breast_cancer_classification_model.h5")

"""#deployment

"""



import streamlit as st
from PIL import Image
import cv2
import numpy as np
import tensorflow as tf

# Load the pre-trained model
model = tf.keras.models.load_model("breast_cancer_classification_model.h5")

def manual_segmentation(image):
    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply thresholding to create a binary image
    _, binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)

    # Apply edge detection to find contours
    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Draw the contours on the original image
    segmented_image = image.copy()
    cv2.drawContours(segmented_image, contours, -1, (0, 255, 0), 2)

    return segmented_image

def predict_image(image):
    # Preprocess the image for the model
    resized_image = cv2.resize(image, (256, 256))
    img = resized_image / 255.0
    img = np.expand_dims(img, axis=0)

    # Get the model prediction
    prediction = model.predict(img)
    label = np.argmax(prediction[0])
    class_names = ["Normal", "Benign", "Malignant"]
    class_name = class_names[label]

    # Perform manual segmentation
    segmented_image = manual_segmentation(resized_image)

    return class_name, segmented_image

# Landing Page
def show_landing_page():
    st.title("Welcome to the Web App!")
    st.write("Please fill in the form and upload an image.")

    # Create form inputs
    patient_id = st.text_input("Patient ID")
    patient_name = st.text_input("Patient Name")
    referral_doctor = st.text_input("Referral Doctor")
    date = st.text_input("Date")
    ultrasound_tech_name = st.text_input("Ultrasound Tech Name")
    image = st.file_uploader("Upload Image", type=["jpg", "jpeg", "png"])

    # Handle form submission
    if st.button("Submit"):
        if image is not None:
            # Open and process the uploaded image
            img = Image.open(image)
            img_np = np.array(img)

            # Get the prediction and segmented image
            prediction, segmented_img = predict_image(img_np)

            # Display the result page with the form data and uploaded image
            show_result_page(patient_id, patient_name, referral_doctor, date, ultrasound_tech_name, img, prediction, segmented_img)

# Result Page
def show_result_page(patient_id, patient_name, referral_doctor, date, ultrasound_tech_name, image, prediction, segmented_img):
    st.title("Result Page")
    st.write("**Form Data:**")
    st.write(f"Patient ID: {patient_id}")
    st.write(f"Patient Name: {patient_name}")
    st.write(f"Referral Doctor: {referral_doctor}")
    st.write(f"Date: {date}")
    st.write(f"Ultrasound Tech Name: {ultrasound_tech_name}")

    st.write("**Uploaded Image:**")
    st.image(image)

    st.write("**Prediction:**")
    st.write(prediction)

    st.write("**Segmented Image:**")
    st.image(segmented_img)

# Main function to run the app
def main():
    show_landing_page()

if __name__ == "__main__":
    main()



